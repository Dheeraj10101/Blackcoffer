{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "25186d49-9269-4d2c-a7ba-d1661e8493c2",
   "metadata": {},
   "source": [
    "## Instructions:\n",
    "### So my approach is as follows.\n",
    "    Firstly I went through a bunch of websites from the Input file provided.\n",
    "    Then to extract the Title and Text I used Python's Beautiful Soup package.\n",
    "    After extracting the Text, I cleaned it, removed the unnecessary part i.e. Stopwords, Pronouns, etc.\n",
    "    After that I calculated all the required Values\n",
    "    And then Exported the sheet as an excel file"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3798c0d4-bef2-4ac8-9b61-b33154d7033c",
   "metadata": {},
   "source": [
    "## Importing the required libaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ca00815e-8e97-4683-bf90-6792e42e6d7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "import textstat\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ecf0aee-639c-4e8d-84ee-5a985ccc285c",
   "metadata": {},
   "source": [
    "## Reading the excel file which have the URL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "559fbbc2-a9bf-4a15-8d7a-5a1b05a07682",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>URL_ID</th>\n",
       "      <th>URL</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>blackassign0001</td>\n",
       "      <td>https://insights.blackcoffer.com/rising-it-cit...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>blackassign0002</td>\n",
       "      <td>https://insights.blackcoffer.com/rising-it-cit...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>blackassign0003</td>\n",
       "      <td>https://insights.blackcoffer.com/internet-dema...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>blackassign0004</td>\n",
       "      <td>https://insights.blackcoffer.com/rise-of-cyber...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>blackassign0005</td>\n",
       "      <td>https://insights.blackcoffer.com/ott-platform-...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            URL_ID                                                URL\n",
       "0  blackassign0001  https://insights.blackcoffer.com/rising-it-cit...\n",
       "1  blackassign0002  https://insights.blackcoffer.com/rising-it-cit...\n",
       "2  blackassign0003  https://insights.blackcoffer.com/internet-dema...\n",
       "3  blackassign0004  https://insights.blackcoffer.com/rise-of-cyber...\n",
       "4  blackassign0005  https://insights.blackcoffer.com/ott-platform-..."
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_url = pd.read_excel('C:/Users/DHEERAJ/OneDrive/Documents/Desktop/BlackCoffer/Input/Input.xlsx')\n",
    "df_url[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04ab15db-42d5-4828-9231-735880e86b82",
   "metadata": {},
   "source": [
    "## Creating the list of the URL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c36263de-9eec-4246-b679-c6fe5fd66e1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the list of all the URL to navigate using Beautifulsoup\n",
    "url_list = df_url['URL'].tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbd2fa2b-f551-44b9-b243-e6a342a9e598",
   "metadata": {},
   "source": [
    "## Extract the Title and Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "249c0425-023b-470f-a6c4-4c0594d5b414",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we have Extract the Title and Text\n",
    "def extract_content(url):\n",
    "    try:\n",
    "        response = requests.get(url)\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        title = soup.title.text.strip()\n",
    "        section = soup.find('div', class_='td-post-content tagdiv-type')  \n",
    "        text = section.text.strip() if section else 'Section not found'\n",
    "        return title, text\n",
    "    except Exception as e:\n",
    "        print(f\"Error occurred while extracting content from {url}: {e}\")\n",
    "        return None, None\n",
    "\n",
    "titles = []\n",
    "texts = []\n",
    "for url in url_list:\n",
    "    title, text = extract_content(url)\n",
    "    titles.append(title)\n",
    "    texts.append(text)\n",
    "df = pd.DataFrame({'Title': titles, 'Text': texts})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef987c9b-21b6-4cbd-b36b-a229f4d15e55",
   "metadata": {},
   "source": [
    "## Check the Positive words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4b5a7cff-0aff-4516-94a0-c2824576282d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results saved\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Word</th>\n",
       "      <th>Match</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>abundant</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>accessible</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>acclaimed</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>accolades</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>accomplish</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1989</th>\n",
       "      <td>works</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1991</th>\n",
       "      <td>worth</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1994</th>\n",
       "      <td>worthwhile</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1995</th>\n",
       "      <td>worthy</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1996</th>\n",
       "      <td>wow</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>589 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            Word  Match\n",
       "4       abundant   True\n",
       "6     accessible   True\n",
       "8      acclaimed   True\n",
       "11     accolades   True\n",
       "14    accomplish   True\n",
       "...          ...    ...\n",
       "1989       works   True\n",
       "1991       worth   True\n",
       "1994  worthwhile   True\n",
       "1995      worthy   True\n",
       "1996         wow   True\n",
       "\n",
       "[589 rows x 2 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# In this we have check the positive Word\n",
    "excel_file_path = 'Raw_data.xlsx' \n",
    "try:\n",
    "    df_excel = pd.read_excel(excel_file_path)\n",
    "except FileNotFoundError:\n",
    "    print(\"Excel file not found.\")\n",
    "    exit()\n",
    "\n",
    "def read_words_from_text(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        words = file.read().split()\n",
    "    return words\n",
    "\n",
    "text_document_path = 'C:/Users/DHEERAJ/OneDrive/Documents/Desktop/BlackCoffer/Words Database/MasterDictionary-20240331T100457Z-001/MasterDictionary/positive-words.txt' \n",
    "\n",
    "try:\n",
    "    words = read_words_from_text(text_document_path)\n",
    "except FileNotFoundError:\n",
    "    print(\"Text document not found.\")\n",
    "    exit()\n",
    "\n",
    "df_excel['Text'] = df_excel['Text'].str.strip().str.lower() \n",
    "results = []\n",
    "for word in words:\n",
    "    word = word.strip().lower() \n",
    "    pattern = r'\\b{}\\b'.format(re.escape(word)) \n",
    "    if df_excel['Text'].str.contains(pattern, regex=True).any():\n",
    "        results.append((word, True))\n",
    "    else:\n",
    "        results.append((word, False))\n",
    "\n",
    "if results:\n",
    "    output_df = pd.DataFrame(results, columns=['Word', 'Match'])\n",
    "    output_df.to_excel('positive.xlsx', index=False)\n",
    "    print(\"Results saved\")\n",
    "else:\n",
    "    print(\"No results to save.\")\n",
    "\n",
    "df_temp = pd.read_excel('positive.xlsx')\n",
    "df_temp[df_temp[\"Match\"]==True]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc07bd0b-1cad-4c0e-bd37-87a1d49c9b4a",
   "metadata": {},
   "source": [
    "## Check the negative word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "341be695-6bd8-4090-941b-0334030da871",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results saved\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Word</th>\n",
       "      <th>Match</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>abolish</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>abrupt</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>absence</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>abuse</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>adamant</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4745</th>\n",
       "      <td>worsening</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4746</th>\n",
       "      <td>worst</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4754</th>\n",
       "      <td>wreak</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4770</th>\n",
       "      <td>writhe</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4771</th>\n",
       "      <td>wrong</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>690 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           Word  Match\n",
       "3       abolish   True\n",
       "13       abrupt   True\n",
       "16      absence   True\n",
       "23        abuse   True\n",
       "53      adamant   True\n",
       "...         ...    ...\n",
       "4745  worsening   True\n",
       "4746      worst   True\n",
       "4754      wreak   True\n",
       "4770     writhe   True\n",
       "4771      wrong   True\n",
       "\n",
       "[690 rows x 2 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# In this we have check the negative word\n",
    "excel_file_path = 'Raw_data.xlsx' \n",
    "try:\n",
    "    df_excel = pd.read_excel(excel_file_path)\n",
    "except FileNotFoundError:\n",
    "    print(\"Excel file not found.\")\n",
    "    exit()\n",
    "\n",
    "def read_words_from_text(file_path):\n",
    "    with open(file_path, 'r') as file:\n",
    "        words = file.read().split()\n",
    "    return words\n",
    "\n",
    "text_document_path = 'C:/Users/DHEERAJ/OneDrive/Documents/Desktop/BlackCoffer/Words Database/MasterDictionary-20240331T100457Z-001/MasterDictionary/negative-words.txt'  # Update with your text document path\n",
    "\n",
    "try:\n",
    "    words = read_words_from_text(text_document_path)\n",
    "except FileNotFoundError:\n",
    "    print(\"Text document not found.\")\n",
    "    exit()\n",
    "\n",
    "df_excel['Text'] = df_excel['Text'].str.strip().str.lower()  \n",
    "results = []\n",
    "for word in words:\n",
    "    word = word.strip().lower() \n",
    "    pattern = r'\\b{}\\b'.format(re.escape(word))  \n",
    "    if df_excel['Text'].str.contains(pattern, regex=True).any():\n",
    "        results.append((word, True))\n",
    "    else:\n",
    "        results.append((word, False))\n",
    "\n",
    "if results:\n",
    "    output_df = pd.DataFrame(results, columns=['Word', 'Match'])\n",
    "    output_df.to_excel('negative.xlsx', index=False)\n",
    "    print(\"Results saved\")\n",
    "else:\n",
    "    print(\"No results to save.\")\n",
    "df_tem = pd.read_excel('negative.xlsx')\n",
    "df_tem [df_tem[\"Match\"]==True]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a73895a-28e5-4f9c-8f51-d42687793ace",
   "metadata": {},
   "source": [
    "## Count the Word and Sentence "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "03977c06-ced2-4b10-988b-196a17b8dd4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\DHEERAJ\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# We have Count the Word and Sentence in the text  \n",
    "nltk.download('punkt') \n",
    "def count_words(paragraph):\n",
    "    return len(nltk.word_tokenize(paragraph))\n",
    "\n",
    "def count_sentences(paragraph):\n",
    "    return len(nltk.sent_tokenize(paragraph))\n",
    "\n",
    "df['Word Count'] = df['Text'].apply(count_words)\n",
    "df['Sentence Count'] = df['Text'].apply(count_sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1a6e79c-664a-48d5-a11d-a85dfda7aacc",
   "metadata": {},
   "source": [
    "## Count the Syllables Word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "9ec91949-318d-476a-94f6-d8125781a03a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we have count the Syllable word in the text \n",
    "def syllable_count(word):\n",
    "    return textstat.syllable_count(word)\n",
    "\n",
    "def total_syllable_count(sentence):\n",
    "    words = word_tokenize(sentence)\n",
    "    syllable_counts = [syllable_count(word) for word in words]\n",
    "    return sum(syllable_counts)\n",
    "\n",
    "df['Syllable Count'] = df['Text'].apply(total_syllable_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fab8902b-371a-471a-bb32-2c82f5072968",
   "metadata": {},
   "source": [
    "## Removing the Stopword"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "71b9f95b-1288-4cd4-97f5-b151a23ab648",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\DHEERAJ\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# In this we have remowe the Stopword\n",
    "nltk.download('stopwords')\n",
    "stop_words = list(stopwords.words('english'))\n",
    "\n",
    "def remove_stopwords(sentence):\n",
    "    words = word_tokenize(sentence)\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    filtered_words = [word for word in words if word.lower() not in stop_words]\n",
    "    filtered_sentence = ' '.join(filtered_words)\n",
    "    return filtered_sentence\n",
    "\n",
    "df['cleaned_sentences'] = df['Text'].apply(remove_stopwords)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ccb5818-5be0-4f0e-8d2a-466fa7c998a3",
   "metadata": {},
   "source": [
    "## Removing the Punctuations Symbols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "82af1ade-ffdb-4f6c-820c-56793caa1ecb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\DHEERAJ\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# In this we have remove the  punctuations symbols \n",
    "nltk.download('punkt')\n",
    "\n",
    "def remove_symbols(sentence):\n",
    "    words = word_tokenize(sentence)\n",
    "    pattern = re.compile(r'[^a-zA-Z0-9\\s]')\n",
    "    filtered_words = [pattern.sub('', word) for word in words]\n",
    "    filtered_text = ' '.join(filtered_words)\n",
    "    return filtered_text\n",
    "\n",
    "df['After removing Symbols'] = df['cleaned_sentences'].apply(remove_symbols)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f51b6288-19f7-4f8b-be6f-ef9f41706ac0",
   "metadata": {},
   "source": [
    "## Count the Character in text "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "eb5e7bef-46ef-497e-acb1-35fdcf8f204c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# In this we have count the character in text\n",
    "def count_letters(paragraph):\n",
    "    return len(paragraph.replace(\" \", \"\"))\n",
    "\n",
    "df['letter_count'] = df['After removing Symbols'].apply(count_letters)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e077c06-3926-49be-ad31-9dd0d26e3b14",
   "metadata": {},
   "source": [
    "## Count the Average Sentence Length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f3c7063b-0b35-456f-a6b0-c16053c5381c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# In this we have count the Average sentence length\n",
    "df[\"Avg_sentence_length\"]=df[\"Word Count\"]/df['Sentence Count']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "650d950a-ea1d-4aa8-b956-06cc92f2d8e8",
   "metadata": {},
   "source": [
    "## Count the Complex Word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "04d561fb-5530-42c5-81e3-b2c8af70855f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# In this we have count the Percentage of Complex word\n",
    "df[\"Percentage_Complex\"]=df['Syllable Count']/df['Word Count']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ee92a19-4a32-4e07-bb40-ad5034ce7212",
   "metadata": {},
   "source": [
    "## Count the Fog Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "dd7d84d8-a9bd-4a49-9c93-f2ca5184a13b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# In this we have count the Fog Index\n",
    "df['Fog Index']=0.4*(df['Avg_sentence_length']+df['Percentage_Complex'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "068b5b7c-21b8-441f-b665-38889c07e59d",
   "metadata": {},
   "source": [
    "## Count the Average number of word per sentence "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b7dddf29-26a5-4aa5-82fc-9e63939e4391",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22.84016784283807\n"
     ]
    }
   ],
   "source": [
    "#In this we have count the  Average number of word per sentence\n",
    "sum1=df['Word Count'].sum()\n",
    "sum2=df['Sentence Count'].sum()\n",
    "Result=sum1/sum2\n",
    "print(Result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9da5edf6-3b78-4ea2-b030-76a36b0d5a29",
   "metadata": {},
   "source": [
    "## Count the Pronoun Sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c5a9107d-498f-4425-971b-72434d3818db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# In this we have count the Pronoun sentences\n",
    "pattern = r'\\b(I|we|my|ours|us)\\b'\n",
    "\n",
    "def pro_cnt(text):\n",
    "    matches = re.findall(pattern, text, flags=re.IGNORECASE)\n",
    "    return len(matches)\n",
    "\n",
    "df['Personal_Pronoun_Counts'] = df['Text'].apply(pro_cnt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7271f3f8-7e86-4839-8f8d-34afea6a0b5d",
   "metadata": {},
   "source": [
    "## Count the Average word length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0b0476fe-7d75-47ee-ba94-ec3409d2d10c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# In this we have count the Average word length for Individual text\n",
    "df['Avg_word_length']=((df['letter_count']/df['Word Count']).round(0)).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "33667bb1-f0b0-4952-a105-fc83cf183bd8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Title</th>\n",
       "      <th>Text</th>\n",
       "      <th>Word Count</th>\n",
       "      <th>Sentence Count</th>\n",
       "      <th>Syllable Count</th>\n",
       "      <th>cleaned_sentences</th>\n",
       "      <th>After removing Symbols</th>\n",
       "      <th>letter_count</th>\n",
       "      <th>Avg_sentence_length</th>\n",
       "      <th>Percentage_Complex</th>\n",
       "      <th>Fog Index</th>\n",
       "      <th>Personal_Pronoun_Counts</th>\n",
       "      <th>Avg_word_length</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Rising IT cities and its impact on the economy...</td>\n",
       "      <td>We have seen a huge development and dependence...</td>\n",
       "      <td>1371</td>\n",
       "      <td>78</td>\n",
       "      <td>1803</td>\n",
       "      <td>seen huge development dependence people techno...</td>\n",
       "      <td>seen huge development dependence people techno...</td>\n",
       "      <td>4037</td>\n",
       "      <td>17.576923</td>\n",
       "      <td>1.315098</td>\n",
       "      <td>7.556809</td>\n",
       "      <td>12</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Rising IT Cities and Their Impact on the Econo...</td>\n",
       "      <td>Throughout history, from the industrial revolu...</td>\n",
       "      <td>1689</td>\n",
       "      <td>80</td>\n",
       "      <td>2571</td>\n",
       "      <td>Throughout history , industrial revolution 18t...</td>\n",
       "      <td>Throughout history  industrial revolution 18th...</td>\n",
       "      <td>6341</td>\n",
       "      <td>21.112500</td>\n",
       "      <td>1.522202</td>\n",
       "      <td>9.053881</td>\n",
       "      <td>6</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Internet Demand's Evolution, Communication Imp...</td>\n",
       "      <td>Introduction\\nIn the span of just a few decade...</td>\n",
       "      <td>1218</td>\n",
       "      <td>57</td>\n",
       "      <td>2149</td>\n",
       "      <td>Introduction span decades , internet undergone...</td>\n",
       "      <td>Introduction span decades  internet undergone ...</td>\n",
       "      <td>5417</td>\n",
       "      <td>21.368421</td>\n",
       "      <td>1.764368</td>\n",
       "      <td>9.253116</td>\n",
       "      <td>13</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Rise of Cybercrime and its Effect in upcoming ...</td>\n",
       "      <td>The way we live, work, and communicate has unq...</td>\n",
       "      <td>1219</td>\n",
       "      <td>52</td>\n",
       "      <td>2046</td>\n",
       "      <td>way live , work , communicate unquestionably c...</td>\n",
       "      <td>way live  work  communicate unquestionably cha...</td>\n",
       "      <td>5253</td>\n",
       "      <td>23.442308</td>\n",
       "      <td>1.678425</td>\n",
       "      <td>10.048293</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>OTT platform and its impact on the entertainme...</td>\n",
       "      <td>The year 2040 is poised to witness a continued...</td>\n",
       "      <td>762</td>\n",
       "      <td>39</td>\n",
       "      <td>1176</td>\n",
       "      <td>year 2040 poised witness continued revolution ...</td>\n",
       "      <td>year 2040 poised witness continued revolution ...</td>\n",
       "      <td>3003</td>\n",
       "      <td>19.538462</td>\n",
       "      <td>1.543307</td>\n",
       "      <td>8.432707</td>\n",
       "      <td>6</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>Due to the COVID-19 the repercussion of the en...</td>\n",
       "      <td>Epidemics, in general, have both direct and in...</td>\n",
       "      <td>1215</td>\n",
       "      <td>50</td>\n",
       "      <td>1877</td>\n",
       "      <td>Epidemics , general , direct indirect costs as...</td>\n",
       "      <td>Epidemics  general  direct indirect costs asso...</td>\n",
       "      <td>4584</td>\n",
       "      <td>24.300000</td>\n",
       "      <td>1.544856</td>\n",
       "      <td>10.337942</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>Impact of COVID-19 pandemic on office space an...</td>\n",
       "      <td>COVID 19 has bought the world to its knees. Wi...</td>\n",
       "      <td>1206</td>\n",
       "      <td>38</td>\n",
       "      <td>1621</td>\n",
       "      <td>COVID 19 bought world knees . businesses shut ...</td>\n",
       "      <td>COVID 19 bought world knees  businesses shut  ...</td>\n",
       "      <td>3394</td>\n",
       "      <td>31.736842</td>\n",
       "      <td>1.344113</td>\n",
       "      <td>13.232382</td>\n",
       "      <td>7</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>Contribution of handicrafts (Visual Arts &amp; Lit...</td>\n",
       "      <td>Handicrafts is an art of making crafts by hand...</td>\n",
       "      <td>431</td>\n",
       "      <td>12</td>\n",
       "      <td>672</td>\n",
       "      <td>Handicrafts art making crafts hand India calle...</td>\n",
       "      <td>Handicrafts art making crafts hand India calle...</td>\n",
       "      <td>1782</td>\n",
       "      <td>35.916667</td>\n",
       "      <td>1.559165</td>\n",
       "      <td>14.990333</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>How COVID-19 is impacting payment preferences?...</td>\n",
       "      <td>Section not found</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>Section found</td>\n",
       "      <td>Section found</td>\n",
       "      <td>12</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>1.333333</td>\n",
       "      <td>1.733333</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>How will COVID-19 affect the world of work? - ...</td>\n",
       "      <td>Section not found</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>Section found</td>\n",
       "      <td>Section found</td>\n",
       "      <td>12</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>1.333333</td>\n",
       "      <td>1.733333</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows Ã— 13 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Title  \\\n",
       "0   Rising IT cities and its impact on the economy...   \n",
       "1   Rising IT Cities and Their Impact on the Econo...   \n",
       "2   Internet Demand's Evolution, Communication Imp...   \n",
       "3   Rise of Cybercrime and its Effect in upcoming ...   \n",
       "4   OTT platform and its impact on the entertainme...   \n",
       "..                                                ...   \n",
       "95  Due to the COVID-19 the repercussion of the en...   \n",
       "96  Impact of COVID-19 pandemic on office space an...   \n",
       "97  Contribution of handicrafts (Visual Arts & Lit...   \n",
       "98  How COVID-19 is impacting payment preferences?...   \n",
       "99  How will COVID-19 affect the world of work? - ...   \n",
       "\n",
       "                                                 Text  Word Count  \\\n",
       "0   We have seen a huge development and dependence...        1371   \n",
       "1   Throughout history, from the industrial revolu...        1689   \n",
       "2   Introduction\\nIn the span of just a few decade...        1218   \n",
       "3   The way we live, work, and communicate has unq...        1219   \n",
       "4   The year 2040 is poised to witness a continued...         762   \n",
       "..                                                ...         ...   \n",
       "95  Epidemics, in general, have both direct and in...        1215   \n",
       "96  COVID 19 has bought the world to its knees. Wi...        1206   \n",
       "97  Handicrafts is an art of making crafts by hand...         431   \n",
       "98                                  Section not found           3   \n",
       "99                                  Section not found           3   \n",
       "\n",
       "    Sentence Count  Syllable Count  \\\n",
       "0               78            1803   \n",
       "1               80            2571   \n",
       "2               57            2149   \n",
       "3               52            2046   \n",
       "4               39            1176   \n",
       "..             ...             ...   \n",
       "95              50            1877   \n",
       "96              38            1621   \n",
       "97              12             672   \n",
       "98               1               4   \n",
       "99               1               4   \n",
       "\n",
       "                                    cleaned_sentences  \\\n",
       "0   seen huge development dependence people techno...   \n",
       "1   Throughout history , industrial revolution 18t...   \n",
       "2   Introduction span decades , internet undergone...   \n",
       "3   way live , work , communicate unquestionably c...   \n",
       "4   year 2040 poised witness continued revolution ...   \n",
       "..                                                ...   \n",
       "95  Epidemics , general , direct indirect costs as...   \n",
       "96  COVID 19 bought world knees . businesses shut ...   \n",
       "97  Handicrafts art making crafts hand India calle...   \n",
       "98                                      Section found   \n",
       "99                                      Section found   \n",
       "\n",
       "                               After removing Symbols  letter_count  \\\n",
       "0   seen huge development dependence people techno...          4037   \n",
       "1   Throughout history  industrial revolution 18th...          6341   \n",
       "2   Introduction span decades  internet undergone ...          5417   \n",
       "3   way live  work  communicate unquestionably cha...          5253   \n",
       "4   year 2040 poised witness continued revolution ...          3003   \n",
       "..                                                ...           ...   \n",
       "95  Epidemics  general  direct indirect costs asso...          4584   \n",
       "96  COVID 19 bought world knees  businesses shut  ...          3394   \n",
       "97  Handicrafts art making crafts hand India calle...          1782   \n",
       "98                                      Section found            12   \n",
       "99                                      Section found            12   \n",
       "\n",
       "    Avg_sentence_length  Percentage_Complex  Fog Index  \\\n",
       "0             17.576923            1.315098   7.556809   \n",
       "1             21.112500            1.522202   9.053881   \n",
       "2             21.368421            1.764368   9.253116   \n",
       "3             23.442308            1.678425  10.048293   \n",
       "4             19.538462            1.543307   8.432707   \n",
       "..                  ...                 ...        ...   \n",
       "95            24.300000            1.544856  10.337942   \n",
       "96            31.736842            1.344113  13.232382   \n",
       "97            35.916667            1.559165  14.990333   \n",
       "98             3.000000            1.333333   1.733333   \n",
       "99             3.000000            1.333333   1.733333   \n",
       "\n",
       "    Personal_Pronoun_Counts  Avg_word_length  \n",
       "0                        12                3  \n",
       "1                         6                4  \n",
       "2                        13                4  \n",
       "3                         5                4  \n",
       "4                         6                4  \n",
       "..                      ...              ...  \n",
       "95                        4                4  \n",
       "96                        7                3  \n",
       "97                        0                4  \n",
       "98                        0                4  \n",
       "99                        0                4  \n",
       "\n",
       "[100 rows x 13 columns]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f93e785a-24f8-4bb0-b612-b64b939ae7bf",
   "metadata": {},
   "source": [
    "## Sum of letter count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b404e175-d1f3-4140-af4c-c1b5c354cf13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "413775\n"
     ]
    }
   ],
   "source": [
    "# we have sum all letter after cleaning the Text\n",
    "a= df['letter_count'].sum()\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe401e61-dcc1-46de-b229-04ec35f17b38",
   "metadata": {},
   "source": [
    "## Removing the \"es\"and \"ed\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "f949b3b5-7813-42fa-b944-1fd01cb22905",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we have remove the \"es\" and \"ed\" from the text \n",
    "def syllable_count(word):\n",
    "    exceptions = ['es', 'ed']\n",
    "    if word[-2:] in exceptions:\n",
    "        return textstat.syllable_count(word[:-2])\n",
    "    else:\n",
    "        return textstat.syllable_count(word)\n",
    "        \n",
    "def total_syllable_count(sentence):\n",
    "    words = word_tokenize(sentence)\n",
    "    syllable_counts = [syllable_count(word) for word in words]\n",
    "    return sum(syllable_counts)\n",
    "df['Syllable Count Per Word'] = df['Text'].apply(total_syllable_count)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e7c00f6-f766-4b74-b716-59cbfa4dbf07",
   "metadata": {},
   "source": [
    "## Save the Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "40592376-6990-4fc6-a8a8-8935ead04329",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_excel('Output_Data.xlsx', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a739f728-5c18-42f4-bb9c-18d5d9ce0095",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
